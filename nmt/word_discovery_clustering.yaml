defaults:
  experiment:
    model_file: examples/output/<EXP>.mod
    hyp_file: examples/output/<EXP>.hyp
    out_file: examples/output/<EXP>.out
    err_file: examples/output/<EXP>.err
    run_for_epochs: 1
    eval_metrics: bleu
  train:
    default_layer_dim: 512
    restart_trainer: True
    trainer: Adam
    learning_rate: 0.0002
    lr_decay: 0.5
    dev_metrics: wer
    training_corpus: !BilingualTrainingCorpus
      train_src: examples/data/head.ja #train.ja
      train_trg: examples/data/head.en #train.en
      dev_src: examples/data/head.ja #dev.ja
      dev_trg: examples/data/head.en #dev.en
    corpus_parser: !BilingualCorpusParser
      src_reader: !PlainTextReader {}
      trg_reader: !PlainTextReader {}
    model: !WordDiscoverer
      src_embedder: !SimpleWordEmbedder
        emb_dim: 512
      encoder: !LSTMEncoder
        layers: 1
      attender: !ClusterAttender
        hidden_dim: 512
        state_dim: 512
        input_dim: 512
      trg_embedder: !SimpleWordEmbedder
        emb_dim: 512
      decoder: !MlpSoftmaxDecoder
        layers: 1
        mlp_hidden_dim: 512
      trg_data: !StandardRetrievalDatabase
        reader: !PlainTextReader {}
        database_file: examples/data/head.en
  decode:
    src_file: examples/data/head.ja #test.ja
    report_path: examples/output/<EXP>
  evaluate:
    ref_file: examples/data/head.en #test.en

word_discovery_dropout-0.1:
  train:
    dropout: 0.1
