from copy import deepcopy
import numpy as np
import logging
import os
import json
import scipy.io.wavfile as wavfile
import librosa
from PIL import Image

DEBUG = False
END = '</s>'
NULL = 'NULL'

if os.path.exists("*.log"):
  os.system("rm *.log")

class XNMTPostprocessor():
  def __init__(self, input_dir, is_phoneme=True):
    self.input_dir = input_dir
    self.is_phoneme = is_phoneme

  # Convert the predicted alignment files to the right format for evaluation
  def convert_alignment_file(self, out_file='pred_alignment.json', feat2wav_file=None): 
    all_files = os.listdir(self.input_dir)
    files = []
    for fn in all_files:
      if fn.endswith(".json"):
        files.append(fn)

    files = sorted(files, key=lambda x:int(x.split('.')[-2]))
    alignments = []
    
    feat2wavs = []
    if feat2wav_file:
      with open(feat2wav_file, 'r') as f:
        feat2wavs = json.load(f)
          
    for i, f in enumerate(files):
      if f.split('.')[-1] == 'json':
        fp = open(self.input_dir + f, 'r')
        ali = json.load(fp)
        if type(ali) == list:
          ali = ali[0]

        fp.close()

        if END in ali['src_sent']:
          ali['src_sent'].remove(END)
          # XXX: This will make the file format inconsistent with the one generated by the preprocess.py
          # convert_xnmt_text function; fix this later
          ali['trg_sent'].remove(END)
          ali['alignment'] = ali['alignment'][:-1]
          if NULL not in ali['trg_sent']:
            ali['trg_sent'] = [NULL] + ali['trg_sent']
            ali['alignment'] = [a+1 if a<len(ali['trg_sent'])-1 else 0 for a in ali['alignment']]
          else:
            ali['alignment'] = [a if a<len(ali['trg_sent'])-1 else 0 for a in ali['alignment']]
        
        new_ali = {}
        if not feat2wav_file:
          new_ali = {'index': ali['index'],
                     'is_phoneme': self.is_phoneme,
                     'caption': ali['src_sent'],
                     'image_concepts': ali['trg_sent'],
                     'alignment': ali['alignment'],
                     'attentions': ali['attentions']
                    }
        else:
          feat_len = len(feat2wavs[i])
          ali_len = len(ali['alignment'])
          reduce_factor = ali['reduce_factor']
          new_feat2wav = []
          for i_a in range(ali_len): 
            for i_f in range(feat_len):
              st = feat2wavs[i]['feat2wav'][i_f * reduce_factor][0]
              end = feat2wavs[i]['feat2wav'][max(i_f * reduce_factor + 1, feat_len - 1)][1]
              new_feat2wav.append([st, end])
          
          new_ali = {'index': ali['index'],
                     'is_phoneme': self.is_phoneme,
                     'caption': ali['src_sent'],
                     'image_concepts': ali['trg_sent'],
                     'alignment': ali['alignment'],
                     'attentions': ali['attentions'],
                     'feat2wav': new_feat2wav 
                    }        

        alignments.append(new_ali)
    
    with open(out_file, 'w') as fp:
      json.dump(alignments, fp, indent=4, sort_keys=True)
  
  def convert_retrieval_file(self, in_file, out_file='retrieval_results.txt'):
    fp = open(in_file)
    ret_indices = []
    for line in fp:
      idx_scores = line.strip().split('), (')
      top_indices = []
      for idx_score in idx_scores:
        idx_score = idx_score.replace('[', '')
        idx_score = idx_score.replace('(', '')
        idx_score = idx_score.replace(')', '')
        idx_score = idx_score.replace(']', '')
        idx_score = idx_score.replace(',', '')
        if DEBUG:
          print(idx_score)
        top_indices.append(idx_score.split()[0])
      ret_indices.append(' '.join(top_indices))
    fp.close()
    fp = open(out_file, 'w') 
    fp.write('\n'.join(ret_indices))
    fp.close()

def alignment_to_cluster(ali_file, out_file='cluster.json'):
  def _find_distinct_tokens(data):
    tokens = set()
    for datum in data:
      if 'image_concepts' in datum: 
        tokens = tokens.union(set(datum['image_concepts']))
      elif 'foreign_sent' in datum:
        tokens = tokens.union(set(datum['foreign_sent']))

    return list(tokens)
  
  fp = open(ali_file, 'r')
  align_info_all = json.load(fp)
  fp.close()
         
  classes = _find_distinct_tokens(align_info_all)
  clusters = {c:[] for c in classes}
  for align_info in align_info_all:
    sent = align_info['caption']
    concepts = align_info['image_concepts']
    alignment = align_info['alignment'] 

    if align_info['is_phoneme']:
      sent, alignment = _findPhraseFromPhoneme(sent, alignment)
    
    for w_i, a_i in zip(sent, alignment):
      if a_i >= len(concepts):
        if DEBUG:
          print('alignment index: ', align_info['index'])
          print('a_i out of range: ', a_i, concepts)
        a_i = 0
      clusters[concepts[a_i]].append(w_i)
      clusters[concepts[a_i]] = list(set(clusters[concepts[a_i]]))

  with open(out_file, 'w') as fp:
    json.dump(clusters, fp, indent=4, sort_keys=True)

def alignment_to_word_units(alignment_file, phone_corpus,
                                     concept_corpus,
                                     landmark_file = None,
                                     word_unit_file='word_units.wrd',
                                     phone_unit_file='phone_units.phn',
                                     concept2id_file=None, 
                                     include_null = False):
  with open(phone_corpus, 'r') as f_p,\
       open(concept_corpus, 'r') as f_c:
    a_corpus, v_corpus = [], []
    for line in f_p: 
      a_corpus.append(line.strip().split())
    
    for line in f_c:
      v_corpus.append(line.strip().split())
  
  if concept2id_file:
    with open(concept2id_file, 'r') as f:
      concept2id = json.load(f)
  else:
    concept2id = {}
  
  with open(alignment_file, 'r') as f:
    alignments = json.load(f)
  
  word_units = []
  phn_units = []
  # XXX
  for align_info, a_sent, v_sent in zip(alignments, a_corpus, v_corpus):
    if len(concept2id) > 0:
      image_concepts = [concept2id[c] for c in v_sent]
      # print(image_concepts)
    else:
      image_concepts = align_info['image_concepts']
    alignment = align_info['alignment']
    pair_id = 'pair_' + str(align_info['index'])
    # print(pair_id) 
    prev_align_idx = -1
    start = 0
    for t, align_idx in enumerate(alignment):
      if t == 0:
        prev_align_idx = align_idx
      
      phn_units.append('%s %d %d %s\n' % (pair_id, t, t + 1, a_sent[t]))
      if prev_align_idx != align_idx:
        if not include_null and prev_align_idx == 0:
          prev_align_idx = align_idx
          start = t
          continue
        word_units.append('%s %d %d %s\n' % (pair_id, start, t, image_concepts[prev_align_idx]))
        prev_align_idx = align_idx
        start = t
      elif t == len(alignment) - 1:
        if not include_null and prev_align_idx == 0:
          continue
        word_units.append('%s %d %d %s\n' % (pair_id, start, t + 1, image_concepts[prev_align_idx]))
    
  with open(word_unit_file, 'w') as f:
    f.write(''.join(word_units))
  
  with open(phone_unit_file, 'w') as f:
    f.write(''.join(phn_units))      

def alignment_to_word_classes(alignment_file, phone_corpus,
                                   word_class_file='words.class',
                                   landmark_file=None, 
                                   include_null = False):
  f = open(phone_corpus, 'r')
  a_corpus = []
  for line in f: 
    a_corpus.append(line.strip().split())
  f.close()
  with open(alignment_file, 'r') as f:
    alignments = json.load(f)
  
  word_units = {}
  for align_info, a_sent in zip(alignments, a_corpus):
    pair_id = 'pair_' + str(align_info['index'])
    alignment = align_info['alignment']
    
    # Cases: 
    # If the concept labels are not available to the system, use the concept alignment
    # Else use the concept labels
    if 'concept_alignment' in align_info:
      n_concepts = max(alignment) + 1
      concept_alignment = align_info['concept_alignment']
      
      print(concept_alignment)
      image_concepts = [-1]*n_concepts 
      i_prev = -1
      for c, i in zip(concept_alignment, alignment):
        if i != i_prev:
          image_concepts[i] = c
          i_prev = i
      print(image_concepts)
    else:
      image_concepts = align_info['image_concepts']
    
    # print(pair_id) 
    prev_align_idx = -1
    start = 0
    if len(alignment) != len(a_sent):
      print('Warning: length of the alignment not equal to the length of sentence: %d != %d' % (len(alignment), len(a_sent)))
      gap = len(a_sent) - len(alignment)
      if gap > 0:
        print('Extend the alignment by %d ...' % gap)
        last_align_idx = alignment[-1]
        alignment.extend([last_align_idx]*gap)
        # print(len(a_sent), len(alignment))
    for t, (align_idx, phn) in enumerate(zip(alignment, a_sent)):
      if t == 0:
        prev_align_idx = align_idx
      
      if prev_align_idx != align_idx:
        if not include_null and prev_align_idx == 0:
          prev_align_idx = align_idx
          start = t
          continue
        if image_concepts[prev_align_idx] not in word_units:
          word_units[image_concepts[prev_align_idx]] = ['%s %d %d\n' % (pair_id, start, t)]
        else:
          word_units[image_concepts[prev_align_idx]].append('%s %d %d\n' % (pair_id, start, t))
        prev_align_idx = align_idx
        start = t
      elif t == len(alignment) - 1:
        if not include_null and prev_align_idx == 0:
          continue
        if image_concepts[prev_align_idx] not in word_units:
          word_units[image_concepts[prev_align_idx]] = ['%s %d %d\n' % (pair_id, start, t + 1)]
        else:
          word_units[image_concepts[prev_align_idx]].append('%s %d %d\n' % (pair_id, start, t + 1))
    
  with open(word_class_file, 'w') as f:
    for i_c, c in enumerate(word_units):
      #print(i_c, c)
      f.write('Class %d:\n' % i_c)
      f.write(''.join(word_units[c]))
      f.write('\n')

def _findPhraseFromPhoneme(sent, alignment):
  if not hasattr(sent, '__len__') or not hasattr(alignment, '__len__'):
    raise TypeError('sent and alignment should be list')
  if DEBUG:
    print(len(sent), len(alignment))
    print(sent, alignment)
  assert len(sent) == len(alignment)
  cur = alignment[0]
  ws = []
  w_align = []
  w = ''  
  for i, a_i in enumerate(alignment):
    if cur == a_i:
      w = w + ' ' + sent[i]
    else:
      ws.append(w)
      w_align.append(cur)
      w = sent[i]
      cur = a_i
  
  ws.append(w)
  w_align.append(cur)
  
  return ws, w_align

def resample_alignment(alignment_file, src_feat2wavs_file, trg_feat2wavs_file, out_file):
  with open(alignment_file, "r") as f:
    alignments = json.load(f)

  with open(src_feat2wavs_file, "r") as f:
    src_feat2wavs = json.load(f)
  with open(trg_feat2wavs_file, "r") as f:
    trg_feat2wavs = json.load(f)

  src_ids = sorted(src_feat2wavs, key=lambda x:int(x.split("_")[-1]))
  trg_ids = sorted(trg_feat2wavs, key=lambda x:int(x.split("_")[-1]))

  new_alignments = []  
  for i_ali, ali in enumerate(alignments):
    # TODO: make this faster by making the feat_id convention more consistent
    trg_feat2wav = trg_feat2wavs[trg_ids[i_ali]]
    src_feat2wav = src_feat2wavs[src_ids[i_ali]]
    wavLen = max(src_feat2wav[-1][1], trg_feat2wav[-1][1])
    
    # Frames are automatically assigned to the last frame (convenient if the two feat2wavs have different lengths)
    trg_wav2feat = [-1]*wavLen
    
    alignment = ali["alignment"]

    if DEBUG:
      logging.debug("i_ali: " + str(i_ali))
      logging.debug("src_ids, trg_ids: %s %s" % (src_ids[i_ali], trg_ids[i_ali]))
      logging.debug("# of wav frames in src_feat2wav, # of feat frames: %d %d" % (src_feat2wav[-1][1], len(src_feat2wav))) 
      logging.debug("# pf wav frames in trg_feat2wav, # of feat frames: %d %d" % (trg_feat2wav[-1][1], len(trg_feat2wav)))
      logging.debug("# of frames in alignment: " + str(len(alignment)))
      logging.debug("# of frames in trg_feat2wav: " + str(len(trg_feat2wav)))
        
    for i_seg, seg in enumerate(trg_feat2wav):  
      start = seg[0]
      end = seg[1]
      for t in range(start, end):
        trg_wav2feat[t] = i_seg
    
    new_alignment = [0]*len(trg_feat2wav)
    for i_src in range(len(alignment)):
      for i_wav in range(src_feat2wav[i_src][0], src_feat2wav[i_src][1]):
        if i_wav > len(trg_wav2feat) - 1:
          logging.warning("inconsistent wav lens: %d %d" % (src_feat2wav[-1][1], len(trg_wav2feat)))
          break
        i_trg = trg_wav2feat[i_wav]
        new_alignment[i_trg] = alignment[i_src] 

    new_align_info = deepcopy(ali)
    new_align_info["alignment"] = new_alignment
    new_alignments.append(new_align_info)
  
  with open(out_file, "w") as f:
    json.dump(new_alignments, f, sort_keys=True, indent=4)  

def convert_boundary_to_segmentation(binary_boundary_file, frame_boundary_file):
  binary_boundaries = np.load(binary_boundary_file)
  frame_boundaries = []
  for i, b_vec in enumerate(binary_boundaries):
    # print("segmentation %d" % i)
    end_frames = np.nonzero(b_vec)[0]
    frame_boundary = [[0, end_frames[0]]]
    for st, end in zip(end_frames[:-1], end_frames[1:]):
      frame_boundary.append([st, end+1])
    
    # if i < 5: 
    #   print("end_frames: ", end_frames)
    # print("frame_boundary: ", frame_boundary) 
    frame_boundaries.append(np.asarray(frame_boundary))

  np.save(frame_boundary_file, frame_boundaries) 

def convert_landmark_segment_to_10ms_segmentation(landmark_segment_file, landmarks_file, frame_segment_file):
  lm_segments = np.load(landmark_segment_file)
  
  lms = np.load(landmarks_file)
  utt_ids = sorted(lms.keys(), key=lambda x:int(x.split('_')[-1]))
  frame_segments = []
  for i, utt_id in enumerate(utt_ids):
    # print(i, utt_id)
    # print('lm_segment: ', lm_segments[i])
    # print('lms: ', lms['arr_'+str(i)])
    # print('len(lm_segment), len(lms): ', len(lm_segments[i]), len(lms['arr_'+str(i)]))
    lm_segment = lm_segments[i]
    lm = np.insert(lms, 0, 0)
    f_seg = []
    for lm_seg in lm_segment:  
      lm = np.insert(lms[utt_id], 0, 0)
      f_seg.append(lm[lm_seg])
    frame_segments.append(np.asarray(f_seg))
  np.save(frame_segment_file, frame_segments)

def convert_10ms_segmentation_to_landmark(segmentation_file, ids_to_utterance_label_file, landmarks_file):
  segmentations = np.load(segmentation_file)
  with open(ids_to_utterance_label_file, "r") as f:
    ids_to_utterance_labels = json.load(f)

  landmarks = {}
  assert len(segmentations) == len(ids_to_utterance_labels)
  for (utt, segmentation) in zip(ids_to_utterance_labels, segmentations):
    if int(utt.split('_')[-1]) >= 1 and int(utt.split('_')[-1]) <= 4:
      print(utt, segmentation[-1])
    landmark = [0]
    for seg in segmentation:
      if seg[1] == seg[0]:
        print("Overlapped boundaries: ", seg)
        continue
      landmark.append(int(seg[1]))
    landmarks[utt] = landmark

  np.savez(landmarks_file, **landmarks)

def convert_sec_to_10ms_landmark(real_time_segment_file, feat2wav_file, frame_segment_file, fs=16000, max_feat_len=2000):
  real_time_segments = np.load(real_time_segment_file) 
  with open(feat2wav_file, 'r') as f:
    feat2wavs = json.load(f)
  
  wav2feats = []
  utt_ids = []
  for utt_id, feat2wav in sorted(feat2wavs.items(), key=lambda x:int(x[0].split('_')[-1])):
    feat_len = len(feat2wav)
    wav_len = feat2wav[-1][1]
    print('utt_id, wav_len: ', utt_id, wav_len)
    wav2feat = np.zeros((wav_len,), dtype=int)
    for i in range(feat_len):
      wav2feat[feat2wav[i][0]:feat2wav[i][1]] = i
    wav2feats.append(wav2feat)
    utt_ids.append(utt_id)

  frame_segments = {}
  max_gap = [0, 0.]
  max_gap_segment = []
  for i_seg, r_seg in enumerate(real_time_segments.tolist()):
    feat_len = len(feat2wavs[utt_ids[i_seg]])
    wav2feat = wav2feats[i_seg]
    wav_len = len(wav2feat)
 
    n_segs = len(r_seg)
    f_seg = [0]
    for i in range(n_segs):
      # XXX: Need thresholding since some syllable segmentation is slightly longer than the actual wavform 
      wav_frame_i = min(int(r_seg[i] * fs), wav_len - 1)
      feat_frame_i = wav2feat[wav_frame_i]
      prev_feat_frame = f_seg[-1]

      # XXX: Prevent the segmentation to move backward
      if feat_frame_i <= 0 or feat_frame_i - prev_feat_frame <= 0:
        continue
      else:
        if feat_frame_i - prev_feat_frame > max_gap[1]:
          max_gap = [i_seg, wav2feat[wav_frame_i] - f_seg[-1]]
          max_gap_segment = [f_seg[-1], wav2feat[wav_frame_i]]
        # XXX: Prevent the segmentation to exceends maxlen / length of the actual feature 
        if feat_frame_i >= min(feat_len - 1, max_feat_len):
          continue
        
        f_seg.append(feat_frame_i + 1) 
    f_seg.append(min(feat_len, max_feat_len))
      
    if i_seg >= 1 and i_seg <= 4:
      print('utt_id: ', utt_ids[i_seg])
      print('feat_len, last segmentation frame: ', feat_len)
      print('wav_len from segmentation, wav_len from wav2feats: ', r_seg[-1] * fs, len(wav2feats[i_seg]))
      print("feat_len from segmentation: ", f_seg[-1])
    
    frame_segments[utt_ids[i_seg]] = f_seg
  
  print("max_gap: ", max_gap)
  print("max_gap landmark: ", max_gap_segment)
  np.savez(frame_segment_file, **frame_segments)

def convert_txt_to_npy_segment(txt_segment_file, npy_segment_file):
  with open(txt_segment_file, 'r') as f:
    lines = f.read().strip().split('\n\n')    
    segmentations = []
    for seg_txt in lines:
      seg = []
      for line in seg_txt.split('\n'):
        t = float(line.split()[1])
        seg.append(t)
      #print(seg)
      segmentations.append(np.asarray(seg))

    np.save(npy_segment_file, segmentations)

def extract_concept_segments(cluster, feat2wav, wav_dir, ids_to_utterance_labels, out_dir=None):
  segments = []
  for seg_info in cluster:
    wav_id = ids_to_utterance[seg_info[0]]
    print(wav_id)
    filename = '_'.join(wav_id.split('_')[:-1]) + ".wav"
    y, sr = wavfile.read(filename)
    
    t_start, t_end = feat2wav[seg_info[1][0]][0], feat2wav[seg_info[1][1]][1]
    if t_end - t_start <= 1600: 
      continue
    segment = y[t_start:t_end]
    segment_melspectrogram = librosa.feature.melspectrogram(segment, sr=sr) 
    
    segments.append(segment)
    segment_features.append(segment_melspectrogram)
    if out_dir:
      segment_file = "%s%s_%d_%d.wav" % (np.out_dir, wav_id, t_start, t_end)
      segment_feat_file = "%s%s_%d_%d.png" % (np.out_dir, wav_id, t_start, t_end)
      wavfile.write(segment_file, sr, y)
      img = Image.fromarray(segment_melspectrogram)
      img.save(segment_feat_file)
  
  return segments, segment_features
     
def extract_top_concept_segments(audio_cluster_file, feat2wav_file, wav_dir, ids_to_utterance_label_file, concept_priors_file, n_top=20):
  with open(audio_cluster_file, "r") as f:
    clusters = json.load(f)
  
  with open(concept_priors_file, "r") as f:
    concept_priors = json.load(f)
  
  with open(ids_to_utterance_label_file, "r") as f:
    ids_to_utterance_labels = json.load(f)

  with open(feat2wav_file, "r") as f:
    feat2wav = json.load(f)

  top_concepts = sorted(concept_priors, key=lambda x:concept_priors[x], reverse=True)[1:n_top+1]

  for c in top_concepts:
    c_dir = "%s%s/" % (out_dir, c) 
    os.mkdir(c_dir)
    _, _ = extract_concept_segments(clusters[c], feat2wav, wav_dir, ids_to_utterance_labels, out_dir=c_dir)

if __name__ == '__main__':
  '''
  postproc = XNMTPostprocessor('../nmt/exp/feb28_phoneme_level_clustering/output/report/')
  postproc.convert_alignment_file('../nmt/exp/feb28_phoneme_level_clustering/output/alignment.json')
  alignment_to_cluster('../nmt/exp/feb28_phoneme_level_clustering/output/alignment.json', '../nmt/exp/feb28_phoneme_level_clustering/output/cluster.json')
  postproc.convert_retrieval_file('../nmt/exp/mar19_phoneme_to_image_norm_over_time/output/phoneme_to_concept.hyp')
  '''
  alignment_file = "../smt/exp/june_24_mfcc_kmeans_mixture=3/flickr30k_pred_alignment.json"
  src_feat2wavs_file = "../data/flickr30k/audio_level/flickr_mfcc_feat2wav.json" 
  ref_feat2wavs_file = "../data/flickr30k/audio_level/flickr30k_gold_alignment.json_feat2wav.json"
  out_file = "../smt/exp/june_24_mfcc_kmeans_mixture=3/pred_alignment_resample.json"
  binary_boundary_file = "../comparison_models/exp/july_20_multimodal_kmeans/boundaries_multimodal-kmeans.npy"
  frame_boundary_file = "../data/flickr30k/audio_level/frame_boundaries_semkmeans.npy"
  txt_syl_segment_file = "../data/flickr30k/audio_level/syllable_boundaries.txt"
  npy_syl_segment_file = "../data/flickr30k/audio_level/syllable_segmentations_osc.npy"
  feat_type = "bn"
  if feat_type == "bn":
    landmark_file = "flickr_landmarks_mbn.npz" #"flickr_landmarks_osc_mbn.npz" #"../data/flickr30k/audio_level/flickr_landmarks.npz" #"flickr_landmarks_mbn.npz" #"../data/flickr30k/audio_level/flickr_landmarks.npz" 
    feat2wavs_htk_file = "../data/flickr30k/audio_level/flickr30k_gold_alignment.json_feat2wav.json"  
  elif feat_type == "mfcc":
    landmark_file = "flickr_landmarks_osc.npz"
    feat2wavs_htk_file = "../data/flickr30k/audio_level/flickr_mfcc_cmvn_htk_feat2wav.json"
  audio_cluster_file = "../comparison_models/exp/aug1_mkmeans/pred_clusters.json"
  ids_to_utterance_file = "../data/flickr30k/audio_level/ids_to_utterance_labels.json"
  wav_dir = "/home/lwang114/data/flickr/flickr_audio/wavs/" 
  gold_seg_file = "../data/flickr30k/audio_level/flickr30k_gold_segmentation_mbn.npy"
  #"../data/flickr30k/audio_level/flickr30k_gold_segmentation_mfcc_htk.npy"
  #convert_boundary_to_segmentation(binary_boundary_file, frame_boundary_file)
  #resample_alignment(alignment_file, src_feat2wavs_file, ref_feat2wavs_file, out_file)
  #convert_txt_to_npy_segment(txt_syl_segment_file, npy_syl_segment_file)
  convert_sec_to_10ms_landmark(npy_syl_segment_file, feat2wavs_htk_file, landmark_file)
  #convert_10ms_segmentation_to_landmark(gold_seg_file, ids_to_utterance_file, "flickr30k_gold_landmarks_mbn.npz")
